{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "![Transformer Model](transformer1.png) \n",
    "\n",
    "This notebook contains the code for the transformer model. The model is based on the paper [Attention is all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani et al. The model is implemented using the [PyTorch](https://pytorch.org/) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# In order to use GPU if you have (Actually for this model having a GPU is not an option, it is a must :D)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model= 256 # AKA embedding size: the size of the token embedding vector\n",
    "nhead = 4 # the number of heads in the MultiHead Attention layers\n",
    "num_encoder_layers = 1 # the number of sub-encoder-layers in the encoder\n",
    "num_decoder_layers = 1 # the number of sub-decoder-layers in the decoder\n",
    "forward_expansion= 4 # the augmention rate of neurons inside the feedforward network in the Transformer encoder/decoder\n",
    "learning_rate = 3e-4 # the learning rate of the Adam optimizer\n",
    "block_size = 128 # the (max) length of the input sequence\n",
    "vocab_size = 30000 # the size of the vocabulary (the number of tokens known by the tokenizer)\n",
    "dropout = 0.25 # the dropout rate of the dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementing blocks of the Transformer\n",
    "\n",
    "#### 3.1. Positional Encoding\n",
    " \n",
    "In this part we are going to use the positional encoding as described in the paper.\n",
    "\n",
    "To do so, we are going to use the following formula :\n",
    "\n",
    "\n",
    "$$PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        position = torch.arange(self.max_sequence_length, device=device, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2, device=device, dtype=torch.float32) * (-math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(self.max_sequence_length, self.d_model, device=device, dtype=torch.float32)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Embedding Block\n",
    " \n",
    "In this part we are going to create the embedding block. This block will be used to convert the input data into a vector representation.\n",
    "\n",
    "This block mainly consists of two parts:\n",
    "1. Token Embedding\n",
    "2. Positional Encoding\n",
    "\n",
    "We implemented the Positional Encoding block as a separate class just above. \n",
    "\n",
    "So we will implement the Embedding Block using this Positional Encoding class and using nn.Embedding class of PyTorch for Token Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, block_size)\n",
    "    def forward(self, x):\n",
    "        out = self.token_embedding(x) + self.positional_encoding()\n",
    "        return  out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Multi Head Attention\n",
    "\n",
    "In this part, we will implement the multi-head attention layer. The multi-head attention layer consists of $h$ attention heads. Each attention head has its own query, key, and value matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # We check if d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # In order to assign the head size in each head we divide d_model by num_heads. This will be the for both d_k and d_v\n",
    "        self.d_qkv = d_model // num_heads\n",
    "\n",
    "        # We use nn.Linear to project the queries, keys, and values. We used the same linear projection for all heads.\n",
    "        # The reason for this is that it allows us to use a single matrix multiplication to project the queries, keys and values\n",
    "        # This is more efficient than using separate matrices for each\n",
    "        self.W_keys = nn.Linear(d_model, d_model)\n",
    "        self.W_queries = nn.Linear(d_model, d_model)\n",
    "        self.W_values = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # We use a single linear projection to project the output of the attention heads\n",
    "        self.linear_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, key_src, query_src, value_src, mask=None):\n",
    "        \n",
    "        # We get the shape of the input batch\n",
    "        B,T,C = key_src.shape # (batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "        # We project the queries, keys and values using their respective weight matrices\n",
    "        keys = self.W_keys(key_src) # (batch_size, seq_len, d_model)\n",
    "        queries = self.W_queries(query_src) # (batch_size, seq_len, d_model)\n",
    "        values = self.W_values(value_src) # (batch_size, seq_len, d_model)\n",
    "        \n",
    "\n",
    "        # We reshape the queries, keys and values so that we can split them into multiple heads\n",
    "        \n",
    "        keys = keys.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "        queries = queries.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "        values = values.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "\n",
    "\n",
    "        # We transpose the queries, keys and values so that the shape of the tensor becomes (batch_size, num_heads, seq_len, d_qkv)\n",
    "\n",
    "        keys = keys.transpose(1,2) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "        queries = queries.transpose(1,2) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "        values = values.transpose(1,2) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "\n",
    "        # We compute the attention scores.\n",
    "        atn_scr = queries @ keys.transpose(-2,-1) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        # We scale the attention scores and apply the mask (if provided)\n",
    "        scaled_atn_scr = atn_scr / self.d_qkv**0.5\n",
    "        if mask is not None:\n",
    "            scaled_atn_scr = scaled_atn_scr.masked_fill(mask==0,float('-inf'))\n",
    "        \n",
    "        # We apply the softmax activation to compute the attention weights\n",
    "        attention_weights = torch.softmax(scaled_atn_scr, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)  # Applying dropout\n",
    "        # Lastly we multiply the attention weights with the values\n",
    "        out = attention_weights @ values\n",
    "        out = out.transpose(1, 2)\n",
    "        # Reshape the matrix to (batch_size, seq_len, d_model) in order to be able to feed it to the next layer\n",
    "        out = out.reshape(B, T, C)\n",
    "        # Apply one last linear projection\n",
    "        out = self.linear_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Feed Forward Neural Network\n",
    "\n",
    "In this part we are going to implement the FFN just like it's in the paper. That mean the net will have 2 linear layer with ReLU activation function between them. The input and the output size of the net will remain the same but inside we will augment the dimension by a factor we assigned called \"Forward Expansion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, forward_expansion, dropout=0.1):  # Added dropout argument\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        # The output size of the first linear layer is forward_expansion time d_model (d_model*forward_expansion)\n",
    "        self.fc1 = nn.Linear(d_model, d_model * forward_expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        # The input size of the second linear layer is d_model * forward_expansion and the output is just d_model \n",
    "        # in order to remain the same size as the input to the feed forward net and be able to use residual connection \n",
    "        self.fc2 = nn.Linear(d_model * forward_expansion, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Added dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)  # Applying dropout\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. Encoder Stack\n",
    "\n",
    "In this part we are going to build the encoder stack. The encoder stack is composed of 1 MHA and 1 FFN. The MHA is followed by a residual connection and a layer normalization. The FFN is also followed by a residual connection and a layer normalization.\n",
    "\n",
    "In the following section we are going to use this block in order to be able o build and Encoder block with multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, forward_expansion, dropout=0.1):  # Added dropout argument\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        self.FFN = FeedForwardNet(d_model=d_model, forward_expansion=forward_expansion, dropout=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Added dropout layer\n",
    "        self.dropout2 = nn.Dropout(dropout)  # Added dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.dropout1(self.MHA(x, x, x))  # Applying dropout\n",
    "        norm_out = self.layer_norm1(out)\n",
    "        out = norm_out + self.dropout2(self.FFN(norm_out))  # Applying dropout\n",
    "        norm_out = self.layer_norm2(out)\n",
    "        return norm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6. Encoder\n",
    "\n",
    "In this part as we are going to build the Encoder block. The Encoder block will contain one embedding layer and Nx Encoder layers (N = num_layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, d_model, num_heads, forward_expansion, num_layers):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.d_model = d_model\n",
    "        self.embeding_layer = EmbeddingLayer(d_model, vocab_size, block_size)\n",
    "        self.layers = nn.ModuleList([EncoderStack(d_model, num_heads, forward_expansion) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeding_layer(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7. Decoder Stack\n",
    "\n",
    "In this part we are going to build the decoder stack. The decoder stack is composed of 2 MHA (one for masked attention and one one for cross attention) and 1 FFN. Each sublayer is followed by residual connection and a layer normalization.\n",
    "\n",
    "In the following section we are going to use this block in order to be able o build and Decoder block with multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderStack(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, forward_expansion, dropout=0.1):  # Added dropout argument\n",
    "        super(DecoderStack, self).__init__()\n",
    "        self.Masked_MHA = MultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        self.Crossed_MHA = MultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout=dropout)\n",
    "        self.FFN = FeedForwardNet(d_model=d_model, forward_expansion=forward_expansion, dropout=dropout)\n",
    "        self.LayerNorm1 = nn.LayerNorm(d_model)\n",
    "        self.LayerNorm2 = nn.LayerNorm(d_model)\n",
    "        self.LayerNorm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)  # Added dropout layer\n",
    "        self.dropout2 = nn.Dropout(dropout)  # Added dropout layer\n",
    "        self.dropout3 = nn.Dropout(dropout)  # Added dropout layer\n",
    "\n",
    "    def forward(self, x, encoder_out, trg_mask):\n",
    "        masked_att_out = self.dropout1(self.Masked_MHA(x, x, x, trg_mask))  # Applying dropout\n",
    "        masked_att_out = self.LayerNorm1(masked_att_out + x)\n",
    "        crossed_att_out = self.dropout2(self.Crossed_MHA(encoder_out, masked_att_out, encoder_out))  # Applying dropout\n",
    "        crossed_att_out = self.LayerNorm2(crossed_att_out + masked_att_out)\n",
    "        ffn_out = self.dropout3(self.FFN(crossed_att_out))  # Applying dropout\n",
    "        ffn_out = self.LayerNorm3(ffn_out + crossed_att_out)\n",
    "        return ffn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8. Decoder\n",
    "\n",
    "In this part as we are going to build the Decoder block. The Decoder block will contain one embedding layer and Nx Decoder layers (N = num_layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size, block_size, d_model, num_heads, forward_expansion, num_layers):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.d_model = d_model\n",
    "        self.embeding_layer = EmbeddingLayer(d_model, vocab_size, block_size)\n",
    "        self.layers = nn.ModuleList([DecoderStack(d_model, num_heads, forward_expansion) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output, trg_mask):\n",
    "        x = self.embeding_layer(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, trg_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9. Transformer\n",
    "\n",
    "Finally we are builtinf the transformer model. The transformer model is composed of the encoder and decoder. The encoder is composed of the embedding layer, and N encoder layers. The decoder is composed of the embedding layer and N decoder layers. In order to make prediction in the end we put a linear layer with the size of the target vocabulary at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size, d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
    "                 forward_expansion, learning_rate, dropout=0.1):  # Added dropout argument\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, block_size, d_model, nhead, forward_expansion, num_encoder_layers)\n",
    "        self.decoder = Decoder(vocab_size, block_size, d_model, nhead, forward_expansion, num_decoder_layers)\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.block_size = block_size\n",
    "        # We defined the mask for the decoder in the init function\n",
    "        self.mask = torch.tril(torch.ones((block_size, block_size))).to(device)\n",
    "\n",
    "        # We defined the linear head in order to get the output of the decoder\n",
    "        self.linear_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.batch_loss = []\n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)  # Added dropout layer\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        B, T = trg.shape\n",
    "        enc_src = self.dropout(self.encoder(src))  # Applying dropout\n",
    "        out = self.dropout(self.decoder(trg, enc_src, self.mask))  # Applying dropout\n",
    "        out = self.linear_head(torch.mean(out, dim=1))\n",
    "        out = out.reshape(B, self.vocab_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End\n",
    "\n",
    "### If you have any questions, please contact me.\n",
    "\n",
    "Mail: i_konak@hotmail.com\n",
    "\n",
    "Linkedin: https://www.linkedin.com/in/ismail-konak/\n",
    "\n",
    "GitHub: https://github.com/IsmailKonak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
