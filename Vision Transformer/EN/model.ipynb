{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "![Vision Transformer Model](visiontransformer1.png)\n",
    "\n",
    "This notebook is a PyTorch implementation of the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) architecture by Alexey Dosovitskiy et al. The model is implemented using PyTorch framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Defining Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 8\n",
    "num_channels = 3\n",
    "num_layers = 6\n",
    "embedding_dim = 768\n",
    "num_heads = 12\n",
    "forward_expansion = 3\n",
    "dropout_rate = 0.1\n",
    "attn_dropout = 0.1\n",
    "image_channels = 3 \n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implementing the blocks of the Vision Transformer (ViT) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts\n",
    "\n",
    "##### 3.1 Embeddings\n",
    "- **3.1.1.** Patch Embeddings\n",
    "- **3.1.2.** Patch+Pos Embeddings\n",
    "\n",
    "##### 3.2. Encoder\n",
    "- **3.2.1.** Multi Head Attention\n",
    "- **3.2.2.** Encoder Stack\n",
    "- **3.2.3.** Encoder\n",
    "\n",
    "##### 3.3. MLP Head\n",
    "- **3.3.1.** Classification Head\n",
    "\n",
    "##### 3.4. **Vision Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. **Patch Embeddings**\n",
    "\n",
    "In order to transform images into sequence we may flatten the whole image but MHA is a quadratic operation so let's say if you use images of size $(224,224)$ and you flatten it you will get a sequence of length $50176$ which is too long for MHA to handle and requires too muck computational power.\n",
    "\n",
    "\n",
    "So we split the images into patches and then flatten them and then pass them through a \"projection\" layer to get the embeddings of the patches.\n",
    "\n",
    "In order to get the embeddings the paper mention 2 ways: Using a linear layer with output size embedding dim or using a conv layer with kernel size and stride size equal to the patch size.\n",
    "\n",
    "I wrote down the code for both of them and you can use either of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CNN\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim):\n",
    "        super().__init__() \n",
    "        image_size = image_size\n",
    "        patch_size = patch_size\n",
    "\n",
    "        self.cnn_proj = nn.Conv2d(in_channels=image_channels, out_channels=embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.cnn_proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "# Using MLP\n",
    "class PatchEmbedding2(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patches = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        self.mlp_proj = nn.Linear(in_features=image_channels*patch_size*patch_size, out_features=embedding_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.patches(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.mlp_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. **Patch+Pos Embeddings**\n",
    "\n",
    "Now that we have our patch embedding it's now time to add the positional embeddings and finally wrap up the embedding block.\n",
    "\n",
    "Important point to mention about this block is the **class token**. It's a learnable parameter prepended to the embedding patch in order to serve as image representation vector later. We will use this vector to perform classification in the last part.\n",
    "\n",
    "The positional embedding is a simple matrix of shape (n_patches+1, embed_dim) that is added to the patch embeddings. (+ 1 because of the class token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPosEmbeddings(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        self.patch_embeddings = PatchEmbedding(image_channels, image_size, patch_size, embedding_dim)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, embedding_dim))\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        patch_emb = self.patch_embeddings(x)\n",
    "        cls_tokens = self.cls.expand(b,-1,-1)\n",
    "        patch_emb_cls = torch.cat((cls_tokens,patch_emb),dim=1)\n",
    "        pos_embed = self.position_embeddings\n",
    "        return self.drop(patch_emb_cls + pos_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. **Multi Head Attention**\n",
    "\n",
    "In this part, we will implement the multi-head attention layer. The multi-head attention layer consists of $h$ attention heads. Each attention head has its own query, key, and value matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, proj_attn=0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # We check if d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # In order to assign the head size in each head we divide d_model by num_heads. This will be the for both d_k and d_v\n",
    "        self.d_qkv = d_model // num_heads\n",
    "\n",
    "        # We use nn.Linear to project the queries, keys, and values. We used the same linear projection for all heads.\n",
    "        # The reason for this is that it allows us to use a single matrix multiplication to project the queries, keys and values\n",
    "        # This is more efficient than using separate matrices for each\n",
    "        self.W_keys = nn.Linear(d_model, d_model)\n",
    "        self.W_queries = nn.Linear(d_model, d_model)\n",
    "        self.W_values = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # We use a single linear projection to project the output of the attention heads\n",
    "        self.linear_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(proj_attn) \n",
    "\n",
    "    def forward(self, key_src, query_src, value_src):\n",
    "        \n",
    "        # We get the shape of the input batch\n",
    "        B,T,C = key_src.shape # (batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "        # We project the queries, keys and values using their respective weight matrices\n",
    "        keys = self.W_keys(key_src) # (batch_size, seq_len, d_model)\n",
    "        queries = self.W_queries(query_src) # (batch_size, seq_len, d_model)\n",
    "        values = self.W_values(value_src) # (batch_size, seq_len, d_model)\n",
    "        \n",
    "\n",
    "        # We reshape the queries, keys and values so that we can split them into multiple heads\n",
    "        \n",
    "        keys = keys.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "        queries = queries.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "        values = values.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "\n",
    "\n",
    "        # We transpose the queries, keys and values so that the shape of the tensor becomes (batch_size, num_heads, seq_len, d_qkv)\n",
    "\n",
    "        keys = keys.permute(0,2,1,3) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "        queries = queries.permute(0,2,1,3) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "        values = values.permute(0,2,1,3) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "\n",
    "        # We compute the attention scores.\n",
    "        atn_scr = queries @ keys.transpose(-2,-1) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        # We scale the attention scores\n",
    "        scaled_atn_scr = atn_scr / self.d_qkv**-0.5\n",
    "        \n",
    "        # We apply the softmax activation to compute the attention weights\n",
    "        attention_weights = torch.softmax(scaled_atn_scr, dim=-1)\n",
    "        attention_weights = self.attn_dropout(attention_weights)  # Applying dropout\n",
    "        # Lastly we multiply the attention weights with the values\n",
    "        out = attention_weights @ values\n",
    "        out = out.transpose(1, 2)\n",
    "        # Reshape the matrix to (batch_size, seq_len, d_model) in order to be able to feed it to the next layer\n",
    "        out = out.reshape(B, T, C)\n",
    "        # Apply one last linear projection\n",
    "        out = self.linear_proj(out)\n",
    "        out = self.proj_dropout(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. **Encoder Stack**\n",
    "\n",
    "In this part we are going to build the encoder stack. The encoder stack is composed of 1 MHA and 1 MLP. The MHA is followed by a residual connection and a layer normalization. The MLP is also followed by a residual connection and a layer normalization.\n",
    "\n",
    "In the following section we are going to use this block in order to be able o build and Encoder block with multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, forward_expansion, dropout_rate, attn_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.MHA = MultiHeadAttention(embedding_dim, num_heads, dropout=attn_dropout, proj_attn=dropout_rate )\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim*forward_expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(embedding_dim*forward_expansion, embedding_dim),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, patch_num, embedding_dim)\n",
    "        z = self.MHA(x, x, x) + x \n",
    "        output = self.MLP(self.layer_norm(z)) + z \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. **Encoder**\n",
    "\n",
    "In this part as we are going to build the Encoder block. The Encoder block will contain the embedding layer and $N$ Encoder layers ($N$ = num_layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, forward_expansion, num_layers, dropout_rate, attn_dropout):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderStack(embedding_dim, num_heads, forward_expansion, dropout_rate, attn_dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(self.layer_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. **Classification Head**\n",
    "\n",
    "In this block we are going to take the output of the encoder and extract the class token and just pass that vector through the classification head. This head will be a simple linear layer . The output of this layer will be a vector of size 2, we will later pass this vector through a softmax layer to get the probabilities of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim, num_heads, forward_expansion, num_layers, num_classes, dropout_rate, attn_dropout):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_size = patch_size\n",
    "        self.embeddings = PatchPosEmbeddings(image_channels, image_size, patch_size, embedding_dim, dropout_rate)\n",
    "        self.encoder = Encoder( embedding_dim, num_heads, forward_expansion, num_layers, dropout_rate, attn_dropout)\n",
    "        self.classifier = ClassificationHead(embedding_dim, num_classes, dropout_rate)\n",
    "        self.emb_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.emb_dropout(x)\n",
    "        x = self.encoder(x)\n",
    "        class_embd = x[:, 0, :]\n",
    "        out = self.classifier(class_embd)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        probs = F.softmax(x, dim=-1)\n",
    "        return torch.argmax(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
