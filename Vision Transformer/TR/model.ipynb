{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer\n",
    "\n",
    "![Vision Transformer Model](visiontransformer1.png)\n",
    "\n",
    "Bu notebookta sizlere bir Vision Transformer (ViT) modeli implement edeceğiz. Transformer mimarisi, Ashish Vaswani ve diğerleri tarafından yazılan [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) makalesine dayanmaktadır. Model, [PyTorch](https://pytorch.org/) kullanılarak implement edilmiştir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 1. Kütüphaneleri içe aktarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2. Hiperparametreleri Tanımlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16 # Fotoğrafı böleceğimiz parçaların boyutu \n",
    "num_layers = 12 # Encoder katman sayısı\n",
    "embedding_dim = 768 # Embedding boyutu\n",
    "num_heads = 12 # MSA başlığı sayısı\n",
    "forward_expansion = 3  # Encoder içindeki ileri beslemeli ağ nöron artış oranı\n",
    "dropout_rate = 0.1 # Dropout oranı\n",
    "attn_dropout = 0.1 # Attention Dropout oranı\n",
    "image_channels = 3 # Fotoğrafın kanal sayısı \n",
    "image_size = 32 # Fotoğrafın boyutu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vision Transformer (ViT) Modeli Blokları"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aşamalar\n",
    "\n",
    "##### 3.1 Embeddings\n",
    "- **3.1.1.** Patch Embeddings\n",
    "- **3.1.2.** Patch+Pos Embeddings\n",
    "\n",
    "##### 3.2. Encoder\n",
    "- **3.2.1.** Multi Head Attention\n",
    "- **3.2.2.** Encoder Stack\n",
    "- **3.2.3.** Encoder\n",
    "\n",
    "##### 3.3. MLP Head\n",
    "- **3.3.1.** Sınıflandırma Başlığı\n",
    "\n",
    "##### 3.4. **Vision Transformer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. **Patch Embeddings**\n",
    "\n",
    "Görseli Encoder'a vermek için öncelikle onu sıralı bir diziye dönüştürüp ondan embeddingler elde etmemiz gerekir. Bunun için görseli (flattening) düzleştirmek bir seçenek olsa da Encoder içinde gerçekleşecek MSA (Multihead Self Attention) işlemi karesel bir işlem olduğu için çok ciddi bir hesaplama maliyeti gerektirir. Örneğin gerçek hayat için çok düşük bir çözünürlük olan $(224,224)$ boyutundaki bir görüntüyü düzleştirirseniz, $50176$ uzunluğunda bir vektör elde ederiz ki bunu hesaplamak çok ciddi bir maliyet gerektirir. \n",
    "\n",
    "Bu sebeple görselleri direkt düzleştirme yerine görseli patch adını verdiğimiz parçalara ayırıyoruz ve ardından bir lineer katmandan geçirerek bunun sonucunda embeddingsleri elde edebiliyoruz. \n",
    "\n",
    "Bu işlemleri gerçekleştirmek adına makalede 2 yöntem öneriliyor: Bunlardan ilki görseli $(P, P)$ boyutunda patchlere ayırmak ve ardından bu patchleri $(N, P^2C)$ boyutunda bir matrise dönüştürüp lineer katmadan geçirerek embeddingsleri elde etmek. İkincisi ise kernel ve stride boyutu $(P, P)$ olan bir Evrişimli Sinir Ağı (Convolutional Neural Network) kullandıktan sonra o ağı düzleştirerek embeddingsleri elde etmek.\n",
    "\n",
    "Her iki yöntem için de gerekli kodu yazdım, ikisini de kullanabilirsiniz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CNN\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim):\n",
    "        super().__init__() \n",
    "        image_size = image_size\n",
    "        patch_size = patch_size\n",
    "\n",
    "        self.cnn_proj = nn.Conv2d(in_channels=image_channels, out_channels=embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.cnn_proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1,2)\n",
    "        return x\n",
    "\n",
    "# Using MLP\n",
    "class PatchEmbedding2(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patches = nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        self.mlp_proj = nn.Linear(in_features=image_channels*patch_size*patch_size, out_features=embedding_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.patches(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.mlp_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. **Patch+Pos Embeddings**\n",
    "\n",
    "Patch embeddinglere ulaştığımıza göre sıra ona konumsal bilgileri de ekleyerek birleştirmekte. Bunun için de bir pos embedding layer oluşturuyoruz. Bu layer, patch embeddinglerin boyutu kadar bir çıktı üretecek. Bu çıktıyı patch embeddinglerle toplayarak patch+pos embeddingleri elde edeceğiz.\n",
    "\n",
    "Burada değinmem gereken çok önemli bir konu ise  **class token** adını verdiğimiz bir ek embedding. Bu vektör, modelin hangi sınıfa ait olduğunu bilmesini sağlayacak. Bu embedding'i da patch+pos embeddingslere ekleyeceğiz.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPosEmbeddings(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        \n",
    "        self.patch_embeddings = PatchEmbedding(image_channels, image_size, patch_size, embedding_dim)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, embedding_dim))\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        patch_emb = self.patch_embeddings(x)\n",
    "        cls_tokens = self.cls.expand(b,-1,-1)\n",
    "        patch_emb_cls = torch.cat((cls_tokens,patch_emb),dim=1)\n",
    "        pos_embed = self.position_embeddings\n",
    "        return self.drop(patch_emb_cls + pos_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. **Multihead Self Attention**\n",
    "\n",
    " Bu bölümde Multihead Self Attention katmanını uygulayacağız. Çoklu kafa dikkat katmanı, $h$ adet dikkat başlığına sahiptir. Her dikkat başlığının ayrı bir sorgu, anahtar ve değer matrisi vardır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1, proj_attn=0.2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # We check if d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # In order to assign the head size in each head we divide d_model by num_heads. This will be the for both d_k and d_v\n",
    "        self.d_qkv = d_model // num_heads\n",
    "\n",
    "        # We use nn.Linear to project the queries, keys, and values. We used the same linear projection for all heads.\n",
    "        # The reason for this is that it allows us to use a single matrix multiplication to project the queries, keys and values\n",
    "        # This is more efficient than using separate matrices for each\n",
    "        self.W_keys = nn.Linear(d_model, d_model)\n",
    "        self.W_queries = nn.Linear(d_model, d_model)\n",
    "        self.W_values = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # We use a single linear projection to project the output of the attention heads\n",
    "        self.linear_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(proj_attn) \n",
    "\n",
    "    def forward(self, key_src, query_src, value_src):\n",
    "        \n",
    "        # We get the shape of the input batch\n",
    "        B,T,C = key_src.shape # (batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "        # We project the queries, keys and values using their respective weight matrices\n",
    "        keys = self.W_keys(key_src) # (batch_size, seq_len, d_model)\n",
    "        queries = self.W_queries(query_src) # (batch_size, seq_len, d_model)\n",
    "        values = self.W_values(value_src) # (batch_size, seq_len, d_model)\n",
    "        \n",
    "\n",
    "        # We reshape the queries, keys and values so that we can split them into multiple heads\n",
    "        \n",
    "        keys = keys.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "        queries = queries.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "        values = values.view(B,T,self.num_heads,self.d_qkv) # (batch_size, seq_len, num_heads, d_qkv)\n",
    "\n",
    "\n",
    "        # We transpose the queries, keys and values so that the shape of the tensor becomes (batch_size, num_heads, seq_len, d_qkv)\n",
    "\n",
    "        keys = keys.permute(0,2,1,3) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "        queries = queries.permute(0,2,1,3) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "        values = values.permute(0,2,1,3) # (batch_size, num_heads, seq_len, d_qkv)\n",
    "\n",
    "        # We compute the attention scores.\n",
    "        atn_scr = queries @ keys.transpose(-2,-1) # (batch_size, num_heads, seq_len, seq_len)\n",
    "        # We scale the attention scores\n",
    "        scaled_atn_scr = atn_scr / self.d_qkv**-0.5\n",
    "        \n",
    "        # We apply the softmax activation to compute the attention weights\n",
    "        attention_weights = torch.softmax(scaled_atn_scr, dim=-1)\n",
    "        attention_weights = self.attn_dropout(attention_weights)  # Applying dropout\n",
    "        # Lastly we multiply the attention weights with the values\n",
    "        out = attention_weights @ values\n",
    "        out = out.transpose(1, 2)\n",
    "        # Reshape the matrix to (batch_size, seq_len, d_model) in order to be able to feed it to the next layer\n",
    "        out = out.reshape(B, T, C)\n",
    "        # Apply one last linear projection\n",
    "        out = self.linear_proj(out)\n",
    "        out = self.proj_dropout(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. **Encoder Stack**\n",
    "\n",
    "Bu bölümde Encoder Stack bloğunu oluşturacağız. Encoder Stack bloğu 1 MSA ve 1 MLP bloğundan oluşur.\n",
    "\n",
    "Bir sonraki bölümde bu bloğu kullanarak çoklu katmanlı Encoder bloğu oluşturacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderStack(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, forward_expansion, dropout_rate, attn_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.MHA = MSA(embedding_dim, num_heads, dropout=attn_dropout, proj_attn=dropout_rate )\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim*forward_expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(embedding_dim*forward_expansion, embedding_dim),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, patch_num, embedding_dim)\n",
    "        z = self.MHA(x, x, x) + x \n",
    "        output = self.MLP(self.layer_norm(z)) + z \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3. **Encoder**\n",
    "\n",
    " Bu bölümde Encoder bloğunu oluşturuyoruz. Encoder bloğu ${N}$ adet Encoder Stack bloğundan (${N}$ = num_layers) oluşur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads, forward_expansion, num_layers, dropout_rate, attn_dropout):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderStack(embedding_dim, num_heads, forward_expansion, dropout_rate, attn_dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(self.layer_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1. **Sınıflandırma Başlığı**\n",
    "\n",
    "Bu blokta Encoder'dan gelen çıktıdan class token'ı çıkarıp bu vektörü sınıflandırma başlığından'den geçireceğiz. Bu başlık basitçe bir lineer katman olacak. Bu katmanın çıktısı 2 boyutlu bir vektör olacak, bu vektöre daha sonra softmax uygulayıp olası sınıfların olasılıklarına ulaşacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim, eps=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_channels, image_size, patch_size, embedding_dim, num_heads, forward_expansion, num_layers, num_classes, dropout_rate, attn_dropout):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.patch_size = patch_size\n",
    "        self.embeddings = PatchPosEmbeddings(image_channels, image_size, patch_size, embedding_dim, dropout_rate)\n",
    "        self.encoder = Encoder( embedding_dim, num_heads, forward_expansion, num_layers, dropout_rate, attn_dropout)\n",
    "        self.classifier = ClassificationHead(embedding_dim, num_classes, dropout_rate)\n",
    "        self.emb_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.emb_dropout(x)\n",
    "        x = self.encoder(x)\n",
    "        class_embd = x[:, 0, :]\n",
    "        out = self.classifier(class_embd)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        probs = F.softmax(x, dim=-1)\n",
    "        return torch.argmax(probs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
