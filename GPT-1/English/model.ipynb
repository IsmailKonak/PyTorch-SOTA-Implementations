{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT (Generative Pre-Trained Transformer) \n",
    "\n",
    "<img src=\"images/gpt_stable_dif.png\" width=\"20%\" height=\"20%\">\n",
    "\n",
    "GPT is a transformer-based language model which is first introduced in the paper [Improving Language Understanding by Generative Pre-Training by Alec Radford et al. in 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).\n",
    "\n",
    "In this notebook we are going to implement the model in PyTorch according to the paper.\n",
    "\n",
    "Note: In addition to its architecture, the model is particularly notable for its training methods. For this reason, it is useful to examine how the model is trained separately, but in this notebook we will focus only on the architecture of the model. In the future, I will try to examine the training methods as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the hyperparameters are set according to the values in the article\n",
    "embed_dim = 768\n",
    "head_nums = 12\n",
    "layer_num = 12\n",
    "batch_size = 64\n",
    "block_size = 512\n",
    "forward_expansion = 4\n",
    "vocab_size = 40000\n",
    "\n",
    "attn_dropout = 0.1\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing GPT Model blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "\n",
    "In this block we are going to implement the multi head attention block. This block is the core of the transformer model.\n",
    "\n",
    "I should note that the implementation of this block does not contain the scaled dot product attention process from scratch because the implementation of PyTorch use the [FlashAttention](https://arxiv.org/abs/2205.14135) implementation (PyTorch >= 2.0) which is more efficient than the naive implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_drop_rate, proj_drop_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # x: (batch_size, block_size, embedding_dim)\n",
    "        qkv = self.qkv(x) # x: (batch_size, block_size, embedding_dim*3)\n",
    "        qkv = qkv.reshape(B,T,3,self.num_heads,self.head_dim).permute(2,0,3,1,4) # x: (3, batch_size, num_heads, block_size, head_dim)\n",
    "        query,key,value = qkv[0],qkv[1],qkv[2] # query, key, value: (batch_size, num_heads, block_size, head_dim)\n",
    "        attn = F.scaled_dot_product_attention(query,key,value, dropout_p=self.attn_drop_rate, is_causal=True) # attn: (batch_size, num_heads, block_size, head_dim)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, C) # attn: (batch_size, block_size, embedding_dim)\n",
    "        out = self.proj(attn) # out: (batch_size, block_size, embedding_dim)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network\n",
    "\n",
    "This block is a simple feed forward network with 2 hidden layers. For the feed-forward network, the paper used 3072 dimensional inner states which we express by the term \"forward expansion\" (embedding_dim*forward_expansion = inner_states). Additionaly we use GELU activation function instead of ReLU as it is shown to perform better in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embedding_dim, forward_expansion, drop_rate):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(embedding_dim, embedding_dim*forward_expansion)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear_2 = nn.Linear(embedding_dim*forward_expansion, embedding_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x) # (batch_size, seq_len, embedding_dim*forward_expansion)\n",
    "        x = self.gelu(x) # (batch_size, seq_len, embedding_dim*forward_expansion)\n",
    "        x = self.linear_2(x) # (batch_size, seq_len, embedding_dim)\n",
    "        out = self.dropout(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, attn_drop_rate, drop_rate):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads,\n",
    "                                       attn_drop_rate=attn_drop_rate, proj_drop_rate=drop_rate)\n",
    "        \n",
    "        self.FFN = FFN(embedding_dim=embed_dim, forward_expansion=forward_expansion, drop_rate=drop_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        mha_out = self.MHA(x) # (batch_size, seq_len, embed_dim)\n",
    "        x = self.layer_norm1(mha_out + x)\n",
    "        ffn_out = self.FFN(x) # (batch_size, seq_len, embed_dim)\n",
    "        out = self.layer_norm2(ffn_out + x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT \n",
    "\n",
    "This is the final block where we wrap up everything together and create a GPT model. Apart from the other difference we have seen in previous blocks the GPT model also differ from the original Transformer by using a different positional embedding. The original Transformer model uses a sinusoidal positional embedding but we use a learned positional embedding as described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size ,block_size, embed_dim, num_heads, num_layers, forward_expansion, attn_drop_rate=0., drop_rate=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim) #  Defining the token embedding layer\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_dim) # Defining the positional embedding layer\n",
    "\n",
    "        self.blocks = nn.ModuleList([GPTBlock(embed_dim, num_heads, forward_expansion, attn_drop_rate, drop_rate) for _ in range(num_layers)]) # Defining the GPT blocks (12 layers in the original paper)\n",
    "\n",
    "        self.linear_head = nn.Linear(embed_dim, vocab_size) # Defining the linear layer to the predictions\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.apply(self._init_weights) # Weights initialization\n",
    "\n",
    "    # We initialize the weights of the model with a normal distribution, with mean 0 and standard deviation 0.02 as in the original paper.\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T = x.shape\n",
    "        token_embeddings = self.token_embed(x) # shape (B,T,C)\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (T)\n",
    "        pos_embeddings = self.pos_embed(pos) # shape (T,C)\n",
    "        x = self.dropout(token_embeddings+pos_embeddings) # shape (B,T,C)\n",
    "        for block in self.blocks:\n",
    "            x = block (x) # shape (B,T,C)\n",
    "        out = self.linear_head(x) # shape (B,T,V) where V is the vocab size\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End \n",
    "\n",
    "If you have any questions, please contact me:\n",
    "\n",
    "- Email: [i_konak@hotmail.com](mailto:i_konak@hotmail.com)\n",
    "- Linkedin: [Ismail Konak](https://www.linkedin.com/in/ismail-konak/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
