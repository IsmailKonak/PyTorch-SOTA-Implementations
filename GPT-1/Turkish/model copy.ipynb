{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT (Generative Pre-Trained Transformer) \n",
    "\n",
    "<img src=\"images/gpt_stable_dif.png\" width=\"20%\" height=\"20%\">\n",
    "\n",
    "GPT, ilk olarak OpenAI tarafından [Improving Language Understanding by Generative Pre-Training by Alec Radford et al. in 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) makalesinde tanıtılan Transformer tabanlı bir dil modelidir.\n",
    "\n",
    "Bu notebook'ta, GPT Modelini PyTorch kullanarak implement edeceğiz\n",
    "\n",
    "Not: Model mimarisine ek olarak özellikle eğitim yöntemleri ile dikkat çekmektedir. Bu nedenle modelin nasıl eğitildiğinin ayrıca incelemesinde fayda var ancak bu notebookta biz sadece modelin mimarisine odaklanacağız. İlerleyen zamanlarda mümkün olduğu kadarıyla eğitim yöntemlerini de incelemeye çalışacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kütüphaneleri içeri aktarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametreleri tanımlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm hiperparametreler makaledeki değerlere göre ayarlanmıştır\n",
    "embed_dim = 768\n",
    "head_nums = 12\n",
    "layer_num = 12\n",
    "batch_size = 64\n",
    "block_size = 512\n",
    "forward_expansion = 4\n",
    "vocab_size = 40000\n",
    "\n",
    "attn_dropout = 0.1\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Modelinin bloklarını oluşturuyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention\n",
    "\n",
    "Bu blokta multi head attention bloğunu implement. Bu blok transformer modelinin en önemli parçasıdır.\n",
    "\n",
    "Bu bloğun implementasyonunda \"*scaled dot product attention*\" işlemini sıfırdan implement etmeyeceğim çünkü PyTorch'un kendi implementasyonu [FlashAttention](https://arxiv.org/abs/2205.14135) implementasyonunu kullanmaktadır (PyTorch >= 2.0) ve bu implementasyon bizim kendi yapacağımız klasik implementasyondan çok daha verimli çalışacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_drop_rate, proj_drop_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape # x: (batch_size, block_size, embedding_dim)\n",
    "        qkv = self.qkv(x) # x: (batch_size, block_size, embedding_dim*3)\n",
    "        qkv = qkv.reshape(B,T,3,self.num_heads,self.head_dim).permute(2,0,3,1,4) # x: (3, batch_size, num_heads, block_size, head_dim)\n",
    "        query,key,value = qkv[0],qkv[1],qkv[2] # query, key, value: (batch_size, num_heads, block_size, head_dim)\n",
    "        attn = F.scaled_dot_product_attention(query,key,value, dropout_p=self.attn_drop_rate, is_causal=True) # attn: (batch_size, num_heads, block_size, head_dim)\n",
    "        attn = attn.transpose(1, 2).contiguous().view(B, T, C) # attn: (batch_size, block_size, embedding_dim)\n",
    "        out = self.proj(attn) # out: (batch_size, block_size, embedding_dim)\n",
    "        out = self.proj_drop(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Network\n",
    "\n",
    "Bu block basitçe 2 gizli katmandan oluşan bir ileri beslemeli ağdır. İleri beslemeli ağ için makalede 3072 boyutlu iç durumlar (**inner_states**) kullanılmıştır. **inner_states** terimi yerine bu implementasyonda daha kolay anlaşılması açısından  \"**forward expansion**\" terimini kullanacağım ( *embedding_dim* ${*}$ *forward_expansion* = *inner_states* ). Ayrıca orjinal Transformer'dan farklı olarak burada ReLU yerine makalede belirtildiği üzere GELU (Gaussian Error Linear Unit) aktivasyon fonksiyonunu kullanacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, embedding_dim, forward_expansion, drop_rate):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(embedding_dim, embedding_dim*forward_expansion)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear_2 = nn.Linear(embedding_dim*forward_expansion, embedding_dim)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x) # (batch_size, seq_len, embedding_dim*forward_expansion)\n",
    "        x = self.gelu(x) # (batch_size, seq_len, embedding_dim*forward_expansion)\n",
    "        x = self.linear_2(x) # (batch_size, seq_len, embedding_dim)\n",
    "        out = self.dropout(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, attn_drop_rate, drop_rate):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadAttention(embed_dim=embed_dim, num_heads=num_heads,\n",
    "                                       attn_drop_rate=attn_drop_rate, proj_drop_rate=drop_rate)\n",
    "        \n",
    "        self.FFN = FFN(embedding_dim=embed_dim, forward_expansion=forward_expansion, drop_rate=drop_rate)\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embed_dim)\n",
    "        mha_out = self.MHA(x) # (batch_size, seq_len, embed_dim)\n",
    "        x = self.layer_norm1(mha_out + x)\n",
    "        ffn_out = self.FFN(x) # (batch_size, seq_len, embed_dim)\n",
    "        out = self.layer_norm2(ffn_out + x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT \n",
    "\n",
    "Son olarak bu blokta her şeyi birleştirip GPT modelini tamamlayacağız. GPT modeli geçmiş bloklarda gördüğümüz çeşitli değişiklikler haricinde ek olarak bi de orijinal Transformer modelinden farklı bir positional embedding kullanması ile ayrılır. Orijinal Transformer modeli sinusoidal positional embedding kullanırken biz bu blokta makalede anlatıldığı gibi öğrenilebilir/öğrenilmiş positional embedding kullanacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size ,block_size, embed_dim, num_heads, num_layers, forward_expansion, attn_drop_rate=0., drop_rate=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim) #  Defining the token embedding layer\n",
    "        self.pos_embed = nn.Embedding(block_size, embed_dim) # Defining the positional embedding layer\n",
    "\n",
    "        self.blocks = nn.ModuleList([GPTBlock(embed_dim, num_heads, forward_expansion, attn_drop_rate, drop_rate) for _ in range(num_layers)]) # Defining the GPT blocks (12 layers in the original paper)\n",
    "\n",
    "        self.linear_head = nn.Linear(embed_dim, vocab_size) # Defining the linear layer to the predictions\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.apply(self._init_weights) # Weights initialization\n",
    "\n",
    "    # We initialize the weights of the model with a normal distribution, with mean 0 and standard deviation 0.02 as in the original paper.\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T = x.shape\n",
    "        token_embeddings = self.token_embed(x) # shape (B,T,C)\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device) # shape (T)\n",
    "        pos_embeddings = self.pos_embed(pos) # shape (T,C)\n",
    "        x = self.dropout(token_embeddings+pos_embeddings) # shape (B,T,C)\n",
    "        for block in self.blocks:\n",
    "            x = block (x) # shape (B,T,C)\n",
    "        out = self.linear_head(x) # shape (B,T,V) where V is the vocab size\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Son \n",
    "\n",
    "Eğer herhangi bir sorunuz olursa bana ulaşabilirsiniz.\n",
    "\n",
    "- Email: [i_konak@hotmail.com](mailto:i_konak@hotmail.com)\n",
    "- Linkedin: [Ismail Konak](https://www.linkedin.com/in/ismail-konak/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
